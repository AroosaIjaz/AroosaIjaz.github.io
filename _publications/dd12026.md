---
title: "Double descent in quantum kernel methods"
collection: publications
permalink: /publications/dd12026
venue: "PRX Quantum"
date: 2026-01-16 
citation: 'M.Kempkes, et al. PRX Quantum 7, 010312, (2026)'
---

[[PrxQuantum2026]]( https://journals.aps.org/prxquantum/abstract/10.1103/cn64-gs6b)

**Popular Summary**

In machine learning, it has long been thought that making models too complex leads to worse performance on new data, leading to a classic U-shaped curve of error versus model size. But recent studies in classical deep learning have turned that idea on its head: sometimes, making models even larger can actually improve performance again. This surprising twist is called “double descent.” Until now, it was not clear if quantum machine learning models—those using the principles of quantum physics—could show the same behavior.

In this study, we show both analytically and experimentally that quantum models can indeed exhibit double descent. We analyzed quantum models that process data using “quantum feature maps,” and we explored how their performance changes as we vary the number of training examples. Using ideas from random matrix theory, we proved that these models can hit a peak in test error as they cross from having too few parameters to having too many, which is a hallmark of double descent. Our experiments on real-world datasets confirmed this effect, showing consistent patterns across different quantum setups.

This discovery has major implications for how we design and scale quantum machine learning systems. It suggests that bigger quantum models might actually perform better than predicted by traditional wisdom, much like their classical counterparts. Future research could explore whether this trend extends to other types of quantum models, and how to harness it to make quantum learning more practical and powerful.
